{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
            {
            "name": "ep",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_DEVICE_MAX_CONNECTIONS": "1"
            },
            "cwd": "${workspaceFolder}/megatron-lm/examples/gpt3",
            "module": "torch.distributed.run",
            "args": [
                "--nproc_per_node", "1",
                "--nnodes", "1",
                "--master_addr", "localhost",
                "--master_port", "6000",
                "../../pretrain_gpt.py",
                    "--expert-model-parallel-size", "1",
                    "--num-experts", "8",
                    "--disable-bias-linear",
                    // "--moe-ffn-hidden-size", ""
                    "--moe-router-topk", "1",
                "--hidden-size", "1536",
                "--num-attention-heads", "16",
                "--num-layers",                "20", //     "40",
                "--seq-length", "1024",
                "--max-position-embeddings", "1024",
                "--attention-backend", "auto",
                    "--micro-batch-size", "4",
                "--global-batch-size", "8",
                "--train-iters", "500000",
                "--weight-decay", "0.1",
                "--adam-beta1", "0.9",
                "--adam-beta2", "0.95",
                "--init-method-std", "0.006",
                "--clip-grad", "1.0",
                "--bf16",
                "--lr", "6.0e-5",
                "--lr-decay-style", "cosine",
                "--min-lr", "6.0e-6",
                "--lr-warmup-fraction", ".001",
                "--lr-decay-iters", "430000",
                "--tensor-model-parallel-size", "1",
                "--pipeline-model-parallel-size", "1",
                "--context-parallel-size", "1",
                "--data-path", "./owt-ds/openwebtext-10k_text_document",
                "--vocab-file", "./owt-ds/gpt2-vocab.json",
                "--merge-file", "./owt-ds/gpt2-merges.txt",
                "--split", "949,50,1",
                "--train-iters", "100",
                "--log-interval", "1",
                "--log-throughput",
                "--save-interval", "0",
                "--eval-interval", "50",
                "--eval-iters", "10",
                "--tensorboard-dir", "./outdir/251129_143520___gpt2-1.2B-1x1gpu_strong_tp1_no-sp_gbs8/tb",
            ]
        },
        {
            "name": "cp",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_DEVICE_MAX_CONNECTIONS": "1"
            },
            "cwd": "${workspaceFolder}/megatron-lm/examples/gpt3",
            "module": "torch.distributed.run",
            "args": [
                    "--nproc_per_node", "2",
                    "--nnodes", "1",
                    "--master_addr", "localhost",
                    "--master_port", "6000",
                    "../../pretrain_gpt.py",
                    "--hidden-size", "1536",
                    "--num-attention-heads", "16",
                    "--num-layers", "40",
                    "--seq-length", "4096",
                    "--max-position-embeddings", "4096",
                    "--attention-backend", "auto",
                    "--micro-batch-size", "4",
                    "--global-batch-size", "4",
                    "--train-iters", "500000",
                    "--weight-decay", "0.1",
                    "--adam-beta1", "0.9",
                    "--adam-beta2", "0.95",
                    "--init-method-std", "0.006",
                    "--clip-grad", "1.0",
                    "--bf16",
                    "--lr", "6.0e-5",
                    "--lr-decay-style", "cosine",
                    "--min-lr", "6.0e-6",
                    "--lr-warmup-fraction", ".001",
                    "--lr-decay-iters", "430000",
                    "--tensor-model-parallel-size", "1",
                    "--pipeline-model-parallel-size", "1",
                    "--context-parallel-size", "2",
                    // "--cp-comm-type", "p2p",
                    "--cp-comm-type", "all_gather",
                    "--data-path", "./owt-ds/openwebtext-10k_text_document",
                    "--vocab-file", "./owt-ds/gpt2-vocab.json",
                    "--merge-file", "./owt-ds/gpt2-merges.txt",
                    "--split", "949,50,1",
                    "--train-iters", "100",
                    "--log-interval", "1",
                    "--log-throughput",
                    "--save-interval", "0",
                    "--eval-interval", "25",
                    "--eval-iters", "10",
                    "--tensorboard-dir", "./outdir/251107_190201___gpt2xl-1x2gpu_gbs4_len4096_cp2/tb"
            ]
        },
        {
            "name": "tp8-gpt-22B",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {},
            "cwd": "${workspaceFolder}/megatron-lm/examples/gpt3",
            "module": "torch.distributed.run",
            "args": [
                "--nproc_per_node", "8",
                "--nnodes", "1",
                "--master_addr", "localhost",
                "--master_port", "6000",
                "../../pretrain_gpt.py",
                "--hidden-size", "6144",
                "--num-attention-heads", "64",
                "--num-layers", "48",
                "--seq-length", "2048",
                "--max-position-embeddings", "2048",
                "--attention-backend", "auto",
                "--micro-batch-size", "4",
                "--global-batch-size", "4",
                "--train-iters", "500000",
                "--weight-decay", "0.1",
                "--adam-beta1", "0.9",
                "--adam-beta2", "0.95",
                "--init-method-std", "0.006",
                "--clip-grad", "1.0",
                "--bf16",
                "--lr", "6.0e-5",
                "--lr-decay-style", "cosine",
                "--min-lr", "6.0e-6",
                "--lr-warmup-fraction", ".001",
                "--lr-decay-iters", "430000",
                "--sequence-parallel",
                "--tensor-model-parallel-size", "8",
                "--pipeline-model-parallel-size", "1",
                "--context-parallel-size", "1",
                "--data-path", "./owt-ds/openwebtext-10k_text_document",
                "--vocab-file", "./owt-ds/gpt2-vocab.json",
                "--merge-file", "./owt-ds/gpt2-merges.txt",
                "--split", "949,50,1",
                "--train-iters", "100",
                "--log-interval", "1",
                "--log-throughput",
                "--save-interval", "0",
                "--eval-interval", "50",
                "--eval-iters", "10",
                "--tensorboard-dir", "./outdir/dev/tb"
            ]
        },
        {
            "name": "pp-gpt2-4.2B",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {},
            "cwd": "${workspaceFolder}/megatron-lm/examples/gpt3",
            "module": "torch.distributed.run",
            "args": [
                "--nproc_per_node", "4", // align this with gbs
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "6000",
                "../../pretrain_gpt.py",
                "--micro-batch-size", "8",
                "--global-batch-size", "8",
                "--pipeline-model-parallel-size", "4",
                // "--num-virtual-stages-per-pipeline-rank", "2",
                                    //    [--microbatch-group-size-per-virtual-pipeline-stage MICROBATCH_GROUP_SIZE_PER_VP_STAGE]
                                                        //    [--overlap-p2p-communication-warmup-flush]
                "--tensor-model-parallel-size", "1",
                "--hidden-size", "2304",
                "--num-attention-heads", "24",
                "--num-layers", "64",
                "--seq-length", "1024",
                "--max-position-embeddings", "1024",
                "--attention-backend", "auto",
                "--train-iters", "500000",
                "--weight-decay", "0.1",
                "--adam-beta1", "0.9",
                "--adam-beta2", "0.95",
                "--init-method-std", "0.006",
                "--clip-grad", "1.0",
                "--bf16",
                "--lr", "6.0e-5",
                "--lr-decay-style", "cosine",
                "--min-lr", "6.0e-6",
                "--lr-warmup-fraction", ".001",
                "--lr-decay-iters", "430000",
                "--data-path", "./owt-ds/openwebtext-10k_text_document",
                "--vocab-file", "./owt-ds/gpt2-vocab.json",
                "--merge-file", "./owt-ds/gpt2-merges.txt",
                "--split", "949,50,1",
                "--log-interval", "1",
                "--save-interval", "10000",
                "--eval-interval", "50",
                "--save", "./sandbox_run/ckpt",
                "--load", "./sandbox_run/ckpt",
                "--eval-iters", "10",
                "--tensorboard-dir", "./sandbox_run/tb",
                // "--wandb-entity", "vchua",
                // "--wandb-project", "mlm-sandbox",
                // "--wandb-exp-name", "251018_180530_weak_tp4_no-sp_4gpus_gbs8_gpt2-4.2B"
            ]
        },
        {
            "name": "gpt2xl-ddp-vs-zero2",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {},
            "cwd": "${workspaceFolder}/megatron-lm/examples/gpt3",
            "module": "torch.distributed.run",
            "args": [
                "--nproc_per_node", "4", // align this with gbs
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "6000",
                "../../pretrain_gpt.py",
                // "--overlap-param-gather", // ablate this
                "--use-distributed-optimizer", // ablate this
                // "--overlap-grad-reduce", // ablate this
                "--global-batch-size", "4", // align this with proc per node
                "--micro-batch-size", "1",
                "--num-layers", "48",
                "--hidden-size", "1600",
                "--num-attention-heads", "25",
                "--seq-length", "1024",
                "--max-position-embeddings", "1024",
                "--attention-backend", "auto",
                "--train-iters", "500000",
                "--weight-decay", "0.1",
                "--adam-beta1", "0.9",
                "--adam-beta2", "0.95",
                "--init-method-std", "0.006",
                "--clip-grad", "1.0",
                "--bf16",
                "--lr", "6.0e-5",
                "--lr-decay-style", "cosine",
                "--min-lr", "6.0e-6",
                "--lr-warmup-fraction", ".001",
                "--lr-decay-iters", "430000",
                "--tensor-model-parallel-size", "1",
                "--pipeline-model-parallel-size", "1",
                "--data-path", "./owt-ds/openwebtext-10k_text_document",
                "--vocab-file", "./owt-ds/gpt2-vocab.json",
                "--merge-file", "./owt-ds/gpt2-merges.txt",
                "--split", "949,50,1",
                "--log-interval", "1",
                "--save-interval", "10000",
                "--eval-interval", "50",
                "--save", "./sandbox_run/ckpt",
                "--load", "./sandbox_run/ckpt",
                "--eval-iters", "10",
                "--tensorboard-dir", "./sandbox_run/tb",
                "--wandb-entity", "vchua",
                "--wandb-project", "mlm-sandbox",
                "--wandb-exp-name", "ddp-vs-zero"
            ]
        },
        {
            "name": "llama3_8b_fp8",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {},
            "cwd": "${workspaceFolder}/megatron-lm/examples/llama",
            "module": "torch.distributed.run",
            "args": [
                "--nproc_per_node", "1",
                "--nnodes", "1",
                "--node_rank", "0",
                "--master_addr", "localhost",
                "--master_port", "6000",
                "../..//pretrain_gpt.py",
                "--use-mcore-models",
                "--num-layers", "32",
                "--hidden-size", "4096",
                "--ffn-hidden-size", "14336",
                "--num-attention-heads", "32",
                "--group-query-attention",
                "--num-query-groups", "8",
                "--kv-channels", "128",
                "--seq-length", "4096",
                "--max-position-embeddings", "4096",
                "--position-embedding-type", "rope",
                "--rotary-base", "1000000",
                "--rotary-percent", "1.0",
                "--attention-dropout", "0.0",
                "--hidden-dropout", "0.0",
                "--swiglu",
                "--init-method-std", "0.0134",
                "--attention-backend", "fused",
                "--apply-layernorm-1p",
                "--untie-embeddings-and-output-weights",
                "--disable-bias-linear",
                "--micro-batch-size", "1",
                "--global-batch-size", "128",
                "--train-samples", "10000",
                "--lr-decay-samples", "1949218748",
                "--lr-warmup-samples", "3906252",
                "--lr", "0.00015",
                "--min-lr", "0.00001",
                "--decoupled-lr", "5.0e-4",
                "--decoupled-min-lr", "4.5e-5",
                "--lr-decay-style", "cosine",
                "--clip-grad", "1.0",
                "--weight-decay", "0.1",
                "--adam-beta1", "0.9",
                "--adam-beta2", "0.95",
                "--bf16",
                "--grad-reduce-in-bf16",
                "--cross-entropy-loss-fusion",
                "--calculate-per-token-loss",
                "--manual-gc",
                "--empty-unused-memory-level", "1",
                "--exit-duration-in-mins", "235",
                "--use-distributed-optimizer",
                "--overlap-grad-reduce",
                "--overlap-param-gather",
                "--fp8-recipe", "mxfp8",
                "--fp8-format", "e4m3",             
                // "--no-fp8-wgrad",        // to test
                // "--fp8-format", "hybrid",          // delayed scaling
                // "--fp8-amax-history-len", "1024",  // delayed scaling
                // "--fp8-amax-compute-algo", "max",  // delayed scaling
                // "--fp8-param-gather",              // delayed scaling
                "--tensor-model-parallel-size", "1",
                "--context-parallel-size", "1",
                "--sequence-parallel",
                "--mock-data",
                "--tokenizer-type", "NullTokenizer",
                "--vocab-size", "128256",
                "--data-cache-path", "./sandbox_run/benchmark_cache_llama3_8b_fp8",
                "--tiktoken-pattern", "v2",
                "--split", "99,1,0",
                "--no-create-attention-mask-in-dataloader",
                "--no-mmap-bin-files",
                "--num-workers", "1",
                "--log-interval", "1",
                "--eval-iters", "32",
                "--eval-interval", "100",
                "--save-interval", "1000",
                "--log-throughput",
                "--profile",
                "--profile-step-start", "4",
                "--profile-step-end", "6",
                "--ckpt-format", "torch_dist",
                "--distributed-timeout-minutes", "60",
                "--save", "./sandbox_run/checkpoints/llama3_8b_fp8",
                "--load", "./sandbox_run/checkpoints/llama3_8b_fp8",
                "--tensorboard-dir", "./sandbox_run/tensorboard_logs/llama3_8b_fp8"
            ]
        },
        {
            "name": "gpt3_345m_debug",
            "type": "debugpy",
            "request": "launch",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {},
            "cwd": "${workspaceFolder}/megatron-lm/examples/gpt3",
            "module": "torch.distributed.run",
            "args": [
                "--nproc_per_node", "1",
                "--nnodes", "1",
                "--master_addr", "localhost",
                "--master_port", "6000",
                "../../pretrain_gpt.py",
                "--recompute-activations",
                "--recompute-granularity", "full",
                "--recompute-method", "uniform",
                "--recompute-num-layers", "1",
                "--num-layers", "12",
                "--hidden-size", "512",
                "--num-attention-heads", "8",
                "--seq-length", "1024",
                "--max-position-embeddings", "1024",
                "--attention-backend", "auto",
                "--micro-batch-size", "1",
                "--global-batch-size", "1536",
                "--train-iters", "500000",
                "--weight-decay", "0.1",
                "--adam-beta1", "0.9",
                "--adam-beta2", "0.95",
                "--init-method-std", "0.006",
                "--clip-grad", "1.0",
                "--bf16",
                "--lr", "6.0e-5",
                "--lr-decay-style", "cosine",
                "--min-lr", "6.0e-6",
                "--lr-warmup-fraction", ".001",
                "--lr-decay-iters", "430000",
                "--tensor-model-parallel-size", "1",
                "--pipeline-model-parallel-size", "1",
                "--data-path", "./owt-ds/openwebtext-10k_text_document",
                "--vocab-file", "./owt-ds/gpt2-vocab.json",
                "--merge-file", "./owt-ds/gpt2-merges.txt",
                "--split", "949,50,1",
                "--log-interval", "5",
                "--save-interval", "10000",
                "--eval-interval", "1000",
                "--save", "./sandbox_run/ckpt",
                "--load", "./sandbox_run/ckpt",
                "--eval-iters", "10",
                "--tensorboard-dir", "./sandbox_run/tb",
            ]
        },
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        }
    ]
}